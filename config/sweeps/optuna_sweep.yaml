# @package _global_

# Base configuration
defaults:
  - override /hydra/sweeper: optuna
  - override /hydra/launcher: joblib

# Enable multirun mode
hydra:
  mode: MULTIRUN
  
  # Customize output directory for multirun
  # This puts each run in a separate numbered subdirectory
  sweep:
    dir: ${training.output_dir}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  
  # Optuna sweeper configuration
  sweeper:
    # Study configuration
    study_name: cathe_optimization
    storage: sqlite:///optuna_studies.db
    direction: maximize  # Use "minimize" for loss, "maximize" for accuracy/f1
    n_trials: 20  # Total number of trials to run
    n_jobs: 1     # Number of parallel jobs (increase if multiple GPUs)
    
    # Random seed for reproducibility
    sampler:
      seed: 42
      
    # Parameters to optimize (with their search spaces)
    params:
      # Model parameters
      model.learning_rate: interval(1e-5, 1e-3, log=true)
      model.weight_decay: interval(1e-6, 1e-4, log=true)
      model.dropout: interval(0.1, 0.5)
      model.hidden_sizes.0: choice(512, 768, 1024, 2048)
      model.hidden_sizes.1: choice(512, 768, 1024, 2048)
      
      # Training parameters
      training.batch_size: choice(128, 256, 512)