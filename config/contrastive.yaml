defaults:
  - _self_
  - data: protT5

model:
  # Architecture
  embedding_dim: ${data.embedding_dim}
  projection_dims: [1024]  # Increase capacity for 3000 classes
  output_dim: 128  # Increase from 128 for better class separation
  dropout: 0.3
  margin: 0.5
  n_neighbors: 1  # take closest neighbor
  val_max_samples: 50000
  # Optimization
  learning_rate: 1e-4 # Target LR *after* warmup
  weight_decay: 1e-4

  # --- Warmup Configuration ---
  use_warmup: true         # Enable warmup
  warmup_steps: 2000         # Number of linear warmup steps
  warmup_start_factor: 0.1 # Start LR = learning_rate * warmup_start_factor
  # --- End Warmup Config ---

  # Learning rate scheduler (ReduceLROnPlateau - used *after* warmup)
  lr_scheduler:
    monitor: "val/knn_balanced_acc"
    mode: "max"
    factor: 0.7
    patience: 3
    min_lr: 1e-8

training:
  # Training parameters
  batch_size: 1024  # large batch size for better triplet mining
  max_epochs: 50  
  accumulate_grad_batches: 4  # effective batch size of 4K
  
  # Monitoring
  monitor_metric: "val/knn_balanced_acc"
  monitor_mode: "max"
  early_stopping_patience: 20
  log_every_n_steps: 100
  
  # Hardware configuration
  num_workers: 16
  accelerator:
    float32_matmul_precision: "high"
    devices: 1
  
  # Training options
  seed: 42
  gradient_clip_val: 1.0
  precision: "16-mixed"

  # Output paths
  output_dir: "outputs_contrastive"
  checkpoint_dir: "${training.output_dir}/checkpoints"
  log_dir: "${training.output_dir}/logs" 