defaults:
  - _self_
  - data: protT5

model:
  # Architecture
  embedding_dim: ${data.embedding_dim}
  projection_dims: [1024]  # Increase capacity for 3000 classes
  output_dim: 128  # Increase from 128 for better class separation
  dropout: 0.3
  n_neighbors: 1  # take closest neighbor
  #val_max_samples: 10000 # Increased from 10k
  # Optimization
  learning_rate: 1e-4
  weight_decay: 1e-4
  # --- Added Warmup ---
  warmup_epochs: 5       # Number of epochs for linear warmup
  warmup_start_factor: 0.1 # Start LR = learning_rate * warmup_start_factor
  # --- Added Visualization ---
  visualization_method: "tsne" # "umap" or "tsne"
  tsne_viz_dir: "results/tsne_plots"
  umap_viz_dir: "results/umap_plots"

training:
  # Training parameters
  batch_size: 1024  # large batch size for better triplet mining
  max_epochs: 100
  accumulate_grad_batches: 1
  cath_levels: [0, 1, 2, 3] # <--- ADD THIS LINE (0=C, 1=A, 2=T, 3=H)

  # Monitoring
  monitor_metric: "val/loss"
  monitor_mode: "min"
  early_stopping_patience: 20 # Keep original patience
  log_every_n_steps: 100

  # Hardware configuration
  num_workers: 16
  accelerator:
    float32_matmul_precision: "high"
    devices: 1

  # Training options
  seed: 42
  gradient_clip_val: 1.0
  precision: "16-mixed"

  # Output paths - These will be dynamically modified per level
  output_dir: "outputs_contrastive_hierarchical" # Base output directory
  # checkpoint_dir: "${training.output_dir}/checkpoints" # Will be set in code
  # log_dir: "${training.output_dir}/logs" # Will be set in code 