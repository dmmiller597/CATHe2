defaults:
  - _self_
  - data: protT5

model:
  # Architecture
  embedding_dim: ${data.embedding_dim}
  projection_dims: [1024]  # Increase capacity for 3000 classes
  output_dim: 128  # Increase from 128 for better class separation
  dropout: 0.3
  n_neighbors: 1  # take closest neighbor
  val_max_samples: 10000
  # Optimization
  learning_rate: 1e-4
  weight_decay: 1e-4
  # --- Added Warmup ---
  warmup_epochs: 5       # Number of epochs for linear warmup
  warmup_start_factor: 0.1 # Start LR = learning_rate * warmup_start_factor

  # Learning rate scheduler
  lr_scheduler:
    monitor: "val/knn_balanced_acc"
    mode: "max"
    factor: 0.5
    patience: 10
    min_lr: 1e-8

training:
  # Training parameters
  batch_size: 256  # large batch size for better triplet mining
  max_epochs: 50  
  accumulate_grad_batches: 4  # effective batch size of 1K
  
  # Monitoring
  monitor_metric: "val/knn_balanced_acc"
  monitor_mode: "max"
  early_stopping_patience: 20
  log_every_n_steps: 100
  
  # Hardware configuration
  num_workers: 16
  accelerator:
    float32_matmul_precision: "high"
    devices: 1
  
  # Training options
  seed: 42
  gradient_clip_val: 1.0
  precision: "16-mixed"

  # Output paths
  output_dir: "outputs_contrastive"
  checkpoint_dir: "${training.output_dir}/checkpoints"
  log_dir: "${training.output_dir}/logs" 