defaults:
  - _self_
  - data: protT5

model:
  # Architecture
  embedding_dim: ${data.embedding_dim}
  projection_dims: [2048, 1024]  # Increase capacity for 3000 classes
  output_dim: 256  # Increase from 128 for better class separation
  dropout: 0.3  # Good balance between regularization and capacity
  margin: 2.0  # Reasonable margin for triplet loss
  n_neighbors: 15  # Increase for better kNN with 3000 classes
  
  # Optimization
  learning_rate: 5e-5  # Slightly lower for stability with larger batches
  weight_decay: 1e-4  # Increased regularization for large dataset

  # Learning rate scheduler
  lr_scheduler:
    monitor: "val/knn_balanced_acc"
    mode: "max"
    factor: 0.7  # Gentler reduction (from 0.5)
    patience: 3  # Reduce patience for faster adaptation
    min_lr: 1e-8

training:
  # Training parameters
  batch_size: 2048  # Increase for better triplet mining
  max_epochs: 50  # Reduced since we have a large dataset
  accumulate_grad_batches: 8  # Increase for effective batch size of 16K
  
  # Monitoring
  monitor_metric: "val/knn_balanced_acc"
  monitor_mode: "max"
  early_stopping_patience: 7  # Allow more exploration before stopping
  log_every_n_steps: 100  # Increase for large dataset
  
  # Hardware configuration
  num_workers: 16  # Increase for faster data loading
  accelerator:
    float32_matmul_precision: "high"
    devices: 1  # Assuming you have multiple GPUs available
  
  # Training options
  seed: 42
  gradient_clip_val: 1.0
  precision: "16-mixed"

  # Output paths
  output_dir: "outputs_contrastive"
  checkpoint_dir: "${training.output_dir}/checkpoints"
  log_dir: "${training.output_dir}/logs" 