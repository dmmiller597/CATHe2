data:
  data_dir: "data"
  train_embeddings: "TED/s30/esmc/esmc_embeddings_train.npz"
  val_embeddings: "TED/s30/esmc/esmc_embeddings_val.npz"
  test_embeddings: "TED/s30/esmc/esmc_embeddings_test.npz"
  train_labels: "TED/s30/esmc/esmc_labels_train.csv"
  val_labels: "TED/s30/esmc/esmc_labels_val.csv"
  test_labels: "TED/s30/esmc/esmc_labels_test.csv"
  sampling_beta: 0.999  # Controls class balancing (0=no reweighting, 1=inverse frequency)

model:
  # Architecture
  embedding_dim: 960
  hidden_sizes: [1024, 1024, 1024]
  dropout: 0.5
  
  # Optimization
  learning_rate: 1e-4
  weight_decay: 5e-5

  # Learning rate scheduler
  lr_scheduler:
    monitor: "val/loss"
    mode: "min"
    factor: 0.5
    patience: 5
    min_lr: 1e-8

training:
  # Training parameters
  batch_size: 512
  max_epochs: 100
  accumulate_grad_batches: 4

  # Monitoring
  monitor_metric: "val/loss"
  monitor_mode: "min"
  early_stopping_patience: 5
  log_every_n_steps: 50
  
  # Hardware configuration
  num_workers: 4
  accelerator:
    float32_matmul_precision: "high"
    devices: 1
  
  # Training options
  seed: 42
  gradient_clip_val: 1.0
  precision: "16-mixed"

  # Output paths
  output_dir: "outputs"
  checkpoint_dir: "${training.output_dir}/checkpoints"
  log_dir: "${training.output_dir}/logs"
  