defaults:
  - _self_
  - data: protT5

data:
  sampling_beta: 1  # Controls class balancing (0=no reweighting, 1=inverse frequency)

model:
  # Architecture
  embedding_dim: ${data.embedding_dim}  # Gets embedding size from selected data config 
  hidden_sizes: [1024, 1024, 1024]
  dropout: 0.3
  
  # Optimization
  learning_rate: 1e-4
  weight_decay: 5e-5

  # Learning rate scheduler (ReduceLROnPlateau)
  lr_scheduler:
    monitor: "val/balanced_acc"  # Metric to monitor for lr reduction
    mode: "max"                  # Reduce LR when metric stops increasing
    factor: 0.5                  # Multiply LR by this factor when plateauing
    patience: 5                  # Number of epochs with no improvement after which LR will be reduced
    min_lr: 1e-8                 # Lower bound on the learning rate

training:
  # Training parameters
  batch_size: 512
  max_epochs: 100
  accumulate_grad_batches: 4

  # Monitoring
  monitor_metric: "val/balanced_acc"
  monitor_mode: "max"
  early_stopping_patience: 5
  log_every_n_steps: 50
  
  # Hardware configuration
  num_workers: 4
  accelerator:
    float32_matmul_precision: "high"
    devices: 1
  
  # Training options
  seed: 42
  gradient_clip_val: 1.0
  precision: "16-mixed"

  # Output paths
  output_dir: "outputs"
  checkpoint_dir: "${training.output_dir}/checkpoints"
  log_dir: "${training.output_dir}/logs"
  