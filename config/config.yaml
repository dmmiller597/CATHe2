data:
  data_dir: "data"
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  train_embeddings: "data/raw/X_train.npz"
  val_embeddings: "data/raw/X_val.npz"
  test_embeddings: "data/raw/X_test.npz"
  train_labels: "data/raw/Y_Train_SF.csv"
  val_labels: "data/raw/Y_Val_SF.csv"
  test_labels: "data/raw/Y_Test_SF.csv"

model:
  embedding_dim: 1024  # Adjust based on your embedding size
  hidden_sizes: [512, 256]
  num_classes: 2373  # Number of CATH superfamilies
  dropout: 0.2
  use_batch_norm: true  # Whether to use batch normalization

training:
  # Basic training parameters
  batch_size: 32
  learning_rate: 0.001
  max_epochs: 100
  early_stopping_patience: 10
  
  # Hardware configuration
  gpus: 1  # Set to 0 for CPU only
  num_workers: 4  # Number of data loading workers
  
  # Advanced training options
  seed: 42  # Random seed for reproducibility
  gradient_clip_val: 1.0  # Gradient clipping value
  accumulate_grad_batches: 1  # Gradient accumulation steps
  precision: 32  # Training precision (16, 32, or 64)
  
  # Logging and checkpointing
  log_every_n_steps: 50  # How often to log metrics
  save_top_k: 3  # Number of best models to save
  monitor_metric: "val_loss"  # Metric to monitor for checkpointing
  monitor_mode: "min"  # Mode for monitoring (min or max) 